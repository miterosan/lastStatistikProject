---
title: "Homework Sheet 4 -- Regression modeling"
date: 'Due: Monday, February 5 by 23:59 CET'
author: "Group 1"
output: 
  html_document:
    toc: true
    toc_depth: 2
    highlight: tango
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, error = F, message = F, warning = F)

```

```{r libraries, include=FALSE, message=FALSE, warning=FALSE}

# package for convenience functions (e.g. ggplot2, dplyr, etc.)
library(tidyverse)

# package for this course
library(aida)

# package for Bayesian regression
library(brms)

# parallel execution of Stan code
options(mc.cores = parallel::detectCores())

# use the aida-theme for plotting
theme_set(theme_aida())

# global color scheme / non-optimized
project_colors = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#000000")

# setting theme colors globally
scale_colour_discrete <- function(...) {
  scale_colour_manual(..., values = project_colors)
}
scale_fill_discrete <- function(...) {
   scale_fill_manual(..., values = project_colors)
} 

# nicer global knitr options
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
                      cache = TRUE, fig.align = 'center')
```

# Instructions

* Create an Rmd-file with your group number (equivalent to Vips group on StudIP) in the 'author' heading and answer the following questions.
* When all answers are ready, 'Knit' the document to produce a HTML file.
* Create a ZIP archive called "IDA_HW04-Group-XYZ.zip" (where 'XYZ' is *your* group number) containing:
   * an R Markdown file "IDA_HW04-Group-XYZ.Rmd"
   * a knitted HTML document "IDA_HW04-Group-XYZ.html"
   * any other files necessary to compile your Rmarkdown to HTML (data, pictures etc.)
* Upload the ZIP archive on Stud.IP in the respective Vips assignment before the deadline (see above). You may upload as many times as you like before the deadline, only your final submission will count.
* If you run `brms::brm` in an R code chunk, make sure to include `results='hide'` as an option to that code chunk to suppress the output of Stan.

# <span style = "color:firebrick">Exercise 1:</span> Difference coding in Mental Chronometry [20 points]

The purpose of this exercise is to better understand coding schemes for categorical factors.
We also would like to get more comfortable with testing hypotheses about regression coefficients in a Bayesian setting.
Towards this end, we will construct a design matrix by hand to implement **simple difference coding**.
We use the non-informative Bayesian regression model to efficiently sample from the posterior and test hypotheses about the Mental Chronometry data.
In particular we are interested in the hypotheses spelled out in Appendix D.1.1.2 of the web-book, namely:

- mean reaction times are lower for data from the 'reaction' block than for the 'goNoGo' block
- mean reaction times are lower for data from the 'goNoGo' block than for the 'discrimination' block

## Ex 1.a Prepare data [1 point]

Create a variable `data_MC_excerpt` that contains all and only the relevant columns from the MC data set.
The dependent variable should be the first column.

**Solution**

```{r}
data_MC_excerpt <- aida::data_MC_cleaned %>% select(RT, submission_id, shape, block)
data_MC_excerpt
```

## Ex 1.b Get the empirical differences between means [2 points]

Compute the following differences:

1. the mean RT for condition `goNoGo` minus the mean RT for condition `reaction`
2. the mean RT for condition `discrimination` minus the mean RT for condition `goNoGo`

and store the result in the variables provided below (keep the brackets so that the values assigned show in the Rmarkdown output):

**Solution:**

```{r}

reactionBlock <- data_MC_excerpt %>% filter(block == "reaction")
discriminationBlock <- data_MC_excerpt %>% filter(block == "discrimination")
nogoBlock <- data_MC_excerpt %>% filter(block == "goNoGo")

(`goNoGo-reaction`       <-  mean(nogoBlock$RT) - mean(reactionBlock$RT) )
(`discrimination-goNoGo` <-  mean(discriminationBlock$RT) - mean(nogoBlock$RT) )



```

## Ex 1.c Build predictor matrix [5 points]

Add three columns to the data tibble `data_MC_excerpt`, like shown below.
These columns should contain the numerical values to obtain so-called **simple difference coding** (as discussed in the initial video of Chapter 14, after ca. 10 minutes).

```{r, eval = F}
data_MC_excerpt <- data_MC_excerpt %>% 
  mutate(
    grand_mean  = ..., 
    `goNoGo-reaction` = ...,
    `discrimination-goNoGo` = ...
  )
```


**Solution:**

```{r}
# YOUR CODE HERE
data_MC_excerpt <- data_MC_excerpt %>% 
  mutate(
    grand_mean  = mean(data_MC_excerpt$RT), 
    `goNoGo-reaction` = `goNoGo-reaction`,
    `discrimination-goNoGo` = `discrimination-goNoGo`
  )
```

## Ex 1.d Run a Bayesian regression model [3 points]

Use the function `aida::get_samples_regression_noninformative` to obtain 10,000 samples from the posterior distribution for a standard linear regression model.
(Hint: Use `as.matrix` to extract the predictor matrix from the tibble you created in the previous step.)

**Solution:**

```{r}
# YOUR CODE HERE
```

## Ex 1.e Summary statistics [2 point]

Use the function `aida::summarize_sample_vector` to show Bayesian summary statistics for the two most relevant parameters (given the hypotheses we are interested in for the Mental Chronometry experiment).

**Solution:**

```{r}
# YOUR CODE HERE
```

## Ex 1.f Interpret your results [7 points]

Interpret these results using a Kruschke-style logic applied to (non-ROPEd) interval-based hypotheses.
In doing so, state clearly what the coefficients encode, what the numerical results of the last step mean, and what you would conclude from all this regarding any potential evidence (or lack thereof) for or against the research hypotheses.

**Solution:**

> YOUR INTERPRETATION HERE

# <span style = "color:firebrick">Exercise 2:</span> Analyzing the King of France [28 points]

The purpose of this exercise is to practice with logistic regression and the interpretation of interaction terms.
The given data from the King of France experiment was collected from students of the 2020-21 edition of this class.
If you want to understand what this experiment is about, read the relevant appendix chapter in the web-book (D.3).

We will be interested in the following research questions:

- **H1**: The latent probability of "TRUE" judgements is higher in C0 (with presupposition) than in C1 (where the presupposition is part of the at-issue / asserted content).
- **H2**: There is no difference in truth-value judgements between C0 (the positive sentence) and C6 (the negative sentence).
- **H3**: The disposition towards "TRUE" judgements is lower for C9 (where the presupposition is topical) than for C10 (where the presupposition is not topical and occurs under negation).

Load the data as follows (also applying some massaging, similar to what's done in the web-book):

```{r}
IDA_data_KoF <- read_csv('data_KoF-IDA-2020.csv') %>% 
  # discard practice trials
  filter(type != "practice") %>% 
  mutate(
    # add a 'condition' variable
    condition = case_when(
      type == "special" ~ "background check",
      type == "main" ~ str_c("Condition ", condition),
      TRUE ~ "filler"
    ) %>% 
      factor( 
        ordered = T,
        levels = c(str_c("Condition ", c(0, 1, 6, 9, 10)), "background check", "filler")
      )
  ) %>% 
  rename(correct_answer = correct)
```

## Ex 2.a Clean the data [3 points]

Just like in the web-book we are going to clean the data by performing the following two steps in sequence:

1. Remove all data from any participant who got more than 50% of the answer to filler material wrong.
2. Remove individual main trials if the corresponding "background check" question was answered wrongly.

Use code from the web-book at will, but be mindful that you may have to make (minor) changes (e.g., variable naming).

Use R code to report how many participants (in Step 1) and trials (in step 2) are excluded.

**Solution:**

```{r}
# YOUR CODE HERE
# look at error rates for filler sentences by subject
# mark every subject with < 0.5 proportion correct

subject_error_rate <- IDA_data_KoF %>% 
    filter(condition == "filler") %>% 
    group_by(submission_id) %>% 
    summarise(
        proportion_correct = mean(correct_answer == response),
        outlier_subject = proportion_correct < 0.5
    ) %>% 
    arrange(proportion_correct) %>%
    filter(outlier_subject == F)

filtered <- IDA_data_KoF %>% filter(submission_id %in% subject_error_rate$submission_id)
sprintf("Removed %d subjects", nrow(IDA_data_KoF) - nrow(filtered))
```
# exclude every critical trial whose 'background' test question was answered wrongly
```{r}
d_cleaned <- 
  d_cleaned %>% 
  # select only the 'background question' trials
  filter(condition == "background check") %>% 
  # is the background question answered correctly?
  mutate(
    background_correct = correct_answer == response
  ) %>%
  # select only the relevant columns
  select(submission_id, vignette, background_correct) %>%
  # right join lines to original data set 
  right_join(d_cleaned, by = c("submission_id", "vignette")) %>% 
  # remove all special trials, as well as main trials with incorrect background check
  filter(startsWith(condition, "Condition") & background_correct == TRUE)
```

## Ex 2.b Plot the data [2 points]

Plot the data just like in the the first part of the web-book's appendix D.3.4 (proportion per condition). 
Use the web-book code as much as you like while making any necessary amendments.

**Solution:**

```{r}
# YOUR CODE HERE
# plot by-subject error rates
subject_error_rate %>% 
  ggplot(aes(x = proportion_correct, color = outlier_subject, shape = outlier_subject)) + 
  geom_jitter(aes(y = ""), width = 0.001) +
  xlab("Poportion of correct answers") + ylab("") + 
  ggtitle("Distribution of proportion of correct answers on filler trials") +
  xlim(0, 1) +
  scale_color_discrete(name = "Outlier") +
  scale_shape_discrete(name = "Outlier")
```

## Ex 2.c Run logistic regression model [5 points]

Run a logistic regression model on the cleaned data, predicting `response` in terms of categorical factor `condition`.
Make sure to also do the following:

- make the ordered factor `condition` a character vector before running the regression model
- change the non-informative default priors for all slope coefficients to a Student's t distribution with 1 degree of freedom, a mean of 0 and a standard deviation of 2 (see web-book Chapter 13.4)
- set the flap `sample_prior = 'yes'` to also sample from the prior
- take 20,000 samples (using parameter `iter` in the function call of `brm`)

Finally, also add a call of `summary(...)` to show the summary of the fitted model in the Rmarkdown output.

**Solution:**

```{r results='hide'}
# YOUR CODE HERE
```

## Ex 2.d Test hypotheses [12 points]

Use the `brms::hypotheses` function to test the three hypotheses stated above.
For each case interpret the results and formulate a conclusion such as "does/does not provide evidence" or "accept/reject hypothesis" according to the results.

### Hypothesis 1: C0 > C1

**Solution:**

```{r}
# YOUR CODE HERE
```

> YOUR INTERPRETATION HERE

### Hypothesis 2: C0 = C6

**Solution:**

```{r}
# YOUR CODE HERE
```

> YOUR INTERPRETATION HERE

### Hypothesis 3: C9 < C10

**Solution:**

```{r}
# YOUR CODE HERE
```

> YOUR INTERPRETATION HERE

## Ex 2.e Interaction with `gender` [6 points]

Let's get nasty!
Let's do what no good scientist should ever do. (We do it for practice and for rubbing it in that this is NOT what you should EVER do.)

Suppose we did not like the outcome of the test for H1 above.
So we are going to run a new analysis (for which there is no *prima facie* rationale) just to see if we cannot possibly squeeze out something that looks like an argument for the conclusion that we so crave to be supported.
Worst of all, we stipulate -completely out of the blue- a potential gender effect on the understanding of linguistic presupposition (say what?).

Obviously, what this exercise is really after is getting practice with interpreting *interaction terms* for categorical regression.
So, look at the code below, try to understand what it does and then inspect the summary of the model fit shown here.
Then answer the following question: based on this analysis and results shown in the summary, is there any reason to believe that (i) there is a difference in the answer behavior between self-identified male and self-identified female participants (i.e., is there a main effect of `gender`?), and (ii) is there any specific way in which gender impacts on the disposition to select "TRUE" in either of the two conditions (i.e., is there an interaction?).
Identify the source of your conclusion by pointing out which numbers (or whatever) you draw on in the summary of the model fit.

```{r, results = 'hide'}
IDA_data_KoF_interaction <- IDA_data_KoF %>% 
  filter(condition %in% c("Condition 0", "Condition 1")) %>% 
  filter(!is.na(gender)) %>% 
  mutate(
    gender = factor(gender)
  ) %>% 
  droplevels() %>% 
  select(response, condition, gender)
  
# apply 'sum' contrasts
contrasts(IDA_data_KoF_interaction$condition) <- contr.sum(2)
contrasts(IDA_data_KoF_interaction$gender) <- contr.sum(2)

# add intelligible name to the new contrast coding
colnames(contrasts(IDA_data_KoF_interaction$condition)) <- ":Cond1"
colnames(contrasts(IDA_data_KoF_interaction$gender)) <- ":male"

fit_brms_KoF_interaction <- brm(
  # predict `response` in terms of `condition`
  formula = response ~ condition * gender,
  # specify which data to use
  data = IDA_data_KoF_interaction,
  # logistic regression
  family = bernoulli(link = "logit"),
  # weakly informative priors (slightly conservative)
  #   for `class = 'b'` (i.e., all slopes)
  prior = prior(student_t(1, 0, 2),  class = 'b'),
  # also collect samples from the prior (for point-valued testing)
  sample_prior = 'yes',
  # take more than the usual samples (for numerical stability of testing)
  iter = 20000
)
```

```{r}
summary(fit_brms_KoF_interaction)
```

**Solution:**

> YOUR INTERPRETATION HERE
